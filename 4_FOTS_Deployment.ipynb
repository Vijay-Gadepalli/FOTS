{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TOPDGQXEzc6I",
    "outputId": "da83423f-89f7-4911-a0d7-e20eb858e2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok==4.1.1\n",
      "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
      "Building wheels for collected packages: pyngrok\n",
      "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15983 sha256=645218cd233c04229872457f1648dd39c22cf90cb338c77b7f9130578524ac91\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
      "Successfully built pyngrok\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-4.1.1\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-0.85.1-py2.py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 28.5 MB/s \n",
      "\u001b[?25hCollecting blinker\n",
      "  Downloading blinker-1.4.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 57.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
      "Collecting gitpython\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 48.5 MB/s \n",
      "\u001b[?25hCollecting watchdog\n",
      "  Downloading watchdog-2.1.3-py3-none-manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 3.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.6.2-py2.py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 46.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
      "Collecting validators\n",
      "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
      "Collecting base58\n",
      "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.0)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
      "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
      "Collecting ipykernel>=5.1.2\n",
      "  Downloading ipykernel-6.0.3-py3-none-any.whl (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 56.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
      "Collecting ipython<8.0,>=7.23.1\n",
      "  Downloading ipython-7.25.0-py3-none-any.whl (786 kB)\n",
      "\u001b[K     |████████████████████████████████| 786 kB 44.0 MB/s \n",
      "\u001b[?25hCollecting importlib-metadata<4\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: jupyter-client<7.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.19-py3-none-any.whl (368 kB)\n",
      "\u001b[K     |████████████████████████████████| 368 kB 51.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<7.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<7.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.1)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.7.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
      "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
      "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
      "Building wheels for collected packages: blinker\n",
      "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13479 sha256=448baa8881aac0d19f22c8a60cee8a0825ee45d59125e6ce716588ea02695e51\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
      "Successfully built blinker\n",
      "Installing collected packages: prompt-toolkit, ipython, importlib-metadata, ipykernel, smmap, gitdb, watchdog, validators, pydeck, gitpython, blinker, base58, streamlit\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 1.0.18\n",
      "    Uninstalling prompt-toolkit-1.0.18:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.18\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 5.5.0\n",
      "    Uninstalling ipython-5.5.0:\n",
      "      Successfully uninstalled ipython-5.5.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.1\n",
      "    Uninstalling importlib-metadata-4.6.1:\n",
      "      Successfully uninstalled importlib-metadata-4.6.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 4.10.1\n",
      "    Uninstalling ipykernel-4.10.1:\n",
      "      Successfully uninstalled ipykernel-4.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.19 which is incompatible.\n",
      "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.0.3 which is incompatible.\n",
      "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.25.0 which is incompatible.\u001b[0m\n",
      "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.18 importlib-metadata-3.10.1 ipykernel-6.0.3 ipython-7.25.0 prompt-toolkit-3.0.19 pydeck-0.6.2 smmap-4.0.0 streamlit-0.85.1 validators-0.18.2 watchdog-2.1.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "IPython",
         "ipykernel",
         "prompt_toolkit"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install pyngrok==4.1.1\n",
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ket1wXpVH04L",
    "outputId": "2ca9724f-0915-4f95-91b8-5d60d9437885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#mounting google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwZl8SjyGONA",
    "outputId": "7092d23a-e208-49ee-875e-28d572369b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from pyngrok import ngrok\n",
    "# mandatory libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#plotting libraries\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#computer vision libraries\n",
    "import cv2\n",
    "from PIL import Image,ImageDraw\n",
    "from PIL import ImagePath\n",
    "\n",
    "#file extracting libraries\n",
    "from zipfile import ZipFile\n",
    "import scipy.io\n",
    "\n",
    "#miscellaneous libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from shapely.geometry import Polygon\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import UpSampling2D,BatchNormalization,Conv2D,MaxPool2D\n",
    "from tensorflow.keras.layers import Activation, Flatten,Reshape,Input,MaxPooling2D\n",
    "from tensorflow.keras.layers import Bidirectional,GRU,LSTM,Dense,Dropout,Lambda\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def polygon_area(poly):\n",
    "    '''\n",
    "    compute area of a polygon\n",
    "    '''\n",
    "    edge = [\n",
    "        (poly[1][0] - poly[0][0]) * (poly[1][1] + poly[0][1]),\n",
    "        (poly[2][0] - poly[1][0]) * (poly[2][1] + poly[1][1]),\n",
    "        (poly[3][0] - poly[2][0]) * (poly[3][1] + poly[2][1]),\n",
    "        (poly[0][0] - poly[3][0]) * (poly[0][1] + poly[3][1])\n",
    "    ]\n",
    "    return np.sum(edge)/2.\n",
    "\n",
    "def check_and_validate_polys(polys, tags, h_w):\n",
    "\n",
    "    \"\"\"Given polys and tags with image height width, return the valid polys and coresponding tags\"\"\"\n",
    "    \n",
    "    (h, w) = h_w\n",
    "    if polys.shape[0] == 0:\n",
    "        return polys\n",
    "    \n",
    "    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w - 1)\n",
    "    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h - 1)\n",
    "\n",
    "    validated_polys = []\n",
    "    validated_tags = []\n",
    "    \n",
    "    for poly, tag in zip(polys, tags):\n",
    "        p_area = polygon_area(poly)\n",
    "        \n",
    "        if abs(p_area) < 1:\n",
    "            continue\n",
    "            \n",
    "        if p_area > 0:\n",
    "            poly = poly[(0, 3, 2, 1), :]\n",
    "            \n",
    "        validated_polys.append(poly)\n",
    "        validated_tags.append(tag)\n",
    "    return np.array(validated_polys), np.array(validated_tags)\n",
    "\n",
    "\n",
    "def shrink_poly(poly, r):\n",
    "    \"\"\"Create shrinked poly for a given poly to produce score map\"\"\"\n",
    "    \n",
    "    R = 0.3 # shrink ratio\n",
    "    # find the longer pair\n",
    "    if np.linalg.norm(poly[0] - poly[1]) + np.linalg.norm(poly[2] - poly[3]) > np.linalg.norm(poly[0] - poly[3]) + np.linalg.norm(poly[1] - poly[2]):\n",
    "\n",
    "        # first move (p0, p1), (p2, p3), then (p0, p3), (p1, p2)\n",
    "        ## p0, p1\n",
    "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
    "        poly[0][0] += R * r[0] * np.cos(theta)\n",
    "        poly[0][1] += R * r[0] * np.sin(theta)\n",
    "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
    "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
    "        ## p2, p3\n",
    "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
    "        poly[3][0] += R * r[3] * np.cos(theta)\n",
    "        poly[3][1] += R * r[3] * np.sin(theta)\n",
    "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
    "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
    "        ## p0, p3\n",
    "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
    "        poly[0][0] += R * r[0] * np.sin(theta)\n",
    "        poly[0][1] += R * r[0] * np.cos(theta)\n",
    "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
    "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
    "        ## p1, p2\n",
    "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
    "        poly[1][0] += R * r[1] * np.sin(theta)\n",
    "        poly[1][1] += R * r[1] * np.cos(theta)\n",
    "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
    "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
    "    else:\n",
    "        ## p0, p3\n",
    "        # print poly\n",
    "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
    "        poly[0][0] += R * r[0] * np.sin(theta)\n",
    "        poly[0][1] += R * r[0] * np.cos(theta)\n",
    "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
    "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
    "        ## p1, p2\n",
    "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
    "        poly[1][0] += R * r[1] * np.sin(theta)\n",
    "        poly[1][1] += R * r[1] * np.cos(theta)\n",
    "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
    "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
    "        ## p0, p1\n",
    "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
    "        poly[0][0] += R * r[0] * np.cos(theta)\n",
    "        poly[0][1] += R * r[0] * np.sin(theta)\n",
    "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
    "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
    "        ## p2, p3\n",
    "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
    "        poly[3][0] += R * r[3] * np.cos(theta)\n",
    "        poly[3][1] += R * r[3] * np.sin(theta)\n",
    "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
    "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
    "\n",
    "    return poly\n",
    "\n",
    "def point_dist_to_line(p1, p2, p3):\n",
    "    \"\"\"compute the distance from p3 to p1-p2\"\"\"\n",
    "    \n",
    "    return np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)    \n",
    "\n",
    "#Find equation of line using two 2D points p1 and p2\n",
    "def fit_line(p1, p2):\n",
    "    '''fit a line ax+by+c = 0'''\n",
    "    if p1[0] == p1[1]:\n",
    "        return [1., 0., -p1[0]]\n",
    "    else:\n",
    "        [k, b] = np.polyfit(p1, p2, deg=1)\n",
    "        return [k, -1., b]\n",
    "\n",
    "\n",
    "\n",
    "#Find Intersection poitn of 2 lines\n",
    "def line_cross_point(line1, line2):\n",
    "    '''line1 0= ax+by+c, compute the cross point of line1 and line2'''\n",
    "    if line1[0] != 0 and line1[0] == line2[0]:\n",
    "        print('Cross point does not exist')\n",
    "        return None\n",
    "    if line1[0] == 0 and line2[0] == 0:\n",
    "        print('Cross point does not exist')\n",
    "        return None\n",
    "    if line1[1] == 0:\n",
    "        x = -line1[2]\n",
    "        y = line2[0] * x + line2[2]\n",
    "    elif line2[1] == 0:\n",
    "        x = -line2[2]\n",
    "        y = line1[0] * x + line1[2]\n",
    "    else:\n",
    "        k1, _, b1 = line1\n",
    "        k2, _, b2 = line2\n",
    "        x = -(b1-b2)/(k1-k2)\n",
    "        y = k1*x + b1\n",
    "    return np.array([x, y], dtype=np.float32)\n",
    "\n",
    "\n",
    "#Get Equation of line that is perpendicular to line passing through a point\n",
    "def line_verticle(line, point):\n",
    "    '''get the verticle line from line across point'''\n",
    "    if line[1] == 0:\n",
    "        verticle = [0, -1, point[1]]\n",
    "    else:\n",
    "        if line[0] == 0:\n",
    "            verticle = [1, 0, -point[0]]\n",
    "        else:\n",
    "            verticle = [-1./line[0], -1, point[1] - (-1/line[0] * point[0])]\n",
    "    return verticle\n",
    "\n",
    "\n",
    "# Convert a parallelogram to rectangle\n",
    "def rectangle_from_parallelogram(poly):\n",
    "    '''\n",
    "    fit a rectangle from a parallelogram\n",
    "    '''\n",
    "    p0, p1, p2, p3 = poly\n",
    "    angle_p0 = np.arccos(np.dot(p1-p0, p3-p0)/(np.linalg.norm(p0-p1) * np.linalg.norm(p3-p0)))\n",
    "    if angle_p0 < 0.5 * np.pi:\n",
    "        if np.linalg.norm(p0 - p1) > np.linalg.norm(p0-p3):\n",
    "            # p0 and p2\n",
    "            ## p0\n",
    "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
    "            p2p3_verticle = line_verticle(p2p3, p0)\n",
    "\n",
    "            new_p3 = line_cross_point(p2p3, p2p3_verticle)\n",
    "            ## p2\n",
    "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
    "            p0p1_verticle = line_verticle(p0p1, p2)\n",
    "\n",
    "            new_p1 = line_cross_point(p0p1, p0p1_verticle)\n",
    "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
    "        else:\n",
    "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
    "            p1p2_verticle = line_verticle(p1p2, p0)\n",
    "\n",
    "            new_p1 = line_cross_point(p1p2, p1p2_verticle)\n",
    "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
    "            p0p3_verticle = line_verticle(p0p3, p2)\n",
    "\n",
    "            new_p3 = line_cross_point(p0p3, p0p3_verticle)\n",
    "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
    "    else:\n",
    "        if np.linalg.norm(p0-p1) > np.linalg.norm(p0-p3):\n",
    "            # p1 and p3\n",
    "            ## p1\n",
    "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
    "            p2p3_verticle = line_verticle(p2p3, p1)\n",
    "\n",
    "            new_p2 = line_cross_point(p2p3, p2p3_verticle)\n",
    "            ## p3\n",
    "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
    "            p0p1_verticle = line_verticle(p0p1, p3)\n",
    "\n",
    "            new_p0 = line_cross_point(p0p1, p0p1_verticle)\n",
    "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
    "        else:\n",
    "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
    "            p0p3_verticle = line_verticle(p0p3, p1)\n",
    "\n",
    "            new_p0 = line_cross_point(p0p3, p0p3_verticle)\n",
    "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
    "            p1p2_verticle = line_verticle(p1p2, p3)\n",
    "\n",
    "            new_p2 = line_cross_point(p1p2, p1p2_verticle)\n",
    "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "#Sorting a rectangle to get all point in clockwies manner\n",
    "def sort_rectangle(poly):\n",
    "    '''sort the four coordinates of the polygon, points in poly should be sorted clockwise'''\n",
    "    # First find the lowest point\n",
    "    p_lowest = np.argmax(poly[:, 1])\n",
    "    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n",
    "        # if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n",
    "        p0_index = np.argmin(np.sum(poly, axis=1))\n",
    "        p1_index = (p0_index + 1) % 4\n",
    "        p2_index = (p0_index + 2) % 4\n",
    "        p3_index = (p0_index + 3) % 4\n",
    "        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n",
    "    else:\n",
    "        # find the point that sits right to the lowest point\n",
    "        p_lowest_right = (p_lowest - 1) % 4\n",
    "        p_lowest_left = (p_lowest + 1) % 4\n",
    "        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\n",
    "        # assert angle > 0\n",
    "        # if angle <= 0:\n",
    "        #     print(angle, poly[p_lowest], poly[p_lowest_right])\n",
    "        if angle/np.pi * 180 > 45:\n",
    "            #this point is p2\n",
    "            p2_index = p_lowest\n",
    "            p1_index = (p2_index - 1) % 4\n",
    "            p0_index = (p2_index - 2) % 4\n",
    "            p3_index = (p2_index + 1) % 4\n",
    "            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\n",
    "        else:\n",
    "            # this point is p3\n",
    "            p3_index = p_lowest\n",
    "            p0_index = (p3_index + 1) % 4\n",
    "            p1_index = (p3_index + 2) % 4\n",
    "            p2_index = (p3_index + 3) % 4\n",
    "            return poly[[p0_index, p1_index, p2_index, p3_index]], angle\n",
    "\n",
    "\n",
    "\n",
    "def restore_rectangle_rbox(origin, geometry):\n",
    "    ''' Resotre rectangle tbox'''\n",
    "    d = geometry[:, :4]\n",
    "    angle = geometry[:, 4]\n",
    "    # for angle > 0\n",
    "    origin_0 = origin[angle >= 0]\n",
    "    d_0 = d[angle >= 0]\n",
    "    angle_0 = angle[angle >= 0]\n",
    "    if origin_0.shape[0] > 0:\n",
    "        p = np.array([np.zeros(d_0.shape[0]), -d_0[:, 0] - d_0[:, 2],\n",
    "                      d_0[:, 1] + d_0[:, 3], -d_0[:, 0] - d_0[:, 2],\n",
    "                      d_0[:, 1] + d_0[:, 3], np.zeros(d_0.shape[0]),\n",
    "                      np.zeros(d_0.shape[0]), np.zeros(d_0.shape[0]),\n",
    "                      d_0[:, 3], -d_0[:, 2]])\n",
    "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
    "\n",
    "        rotate_matrix_x = np.array([np.cos(angle_0), np.sin(angle_0)]).transpose((1, 0))\n",
    "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
    "\n",
    "        rotate_matrix_y = np.array([-np.sin(angle_0), np.cos(angle_0)]).transpose((1, 0))\n",
    "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
    "\n",
    "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "\n",
    "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
    "\n",
    "        p3_in_origin = origin_0 - p_rotate[:, 4, :]\n",
    "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
    "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
    "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
    "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
    "\n",
    "        new_p_0 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
    "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
    "    else:\n",
    "        new_p_0 = np.zeros((0, 4, 2))\n",
    "    # for angle < 0\n",
    "    origin_1 = origin[angle < 0]\n",
    "    d_1 = d[angle < 0]\n",
    "    angle_1 = angle[angle < 0]\n",
    "    if origin_1.shape[0] > 0:\n",
    "        p = np.array([-d_1[:, 1] - d_1[:, 3], -d_1[:, 0] - d_1[:, 2],\n",
    "                      np.zeros(d_1.shape[0]), -d_1[:, 0] - d_1[:, 2],\n",
    "                      np.zeros(d_1.shape[0]), np.zeros(d_1.shape[0]),\n",
    "                      -d_1[:, 1] - d_1[:, 3], np.zeros(d_1.shape[0]),\n",
    "                      -d_1[:, 1], -d_1[:, 2]])\n",
    "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
    "\n",
    "        rotate_matrix_x = np.array([np.cos(-angle_1), -np.sin(-angle_1)]).transpose((1, 0))\n",
    "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
    "\n",
    "        rotate_matrix_y = np.array([np.sin(-angle_1), np.cos(-angle_1)]).transpose((1, 0))\n",
    "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
    "\n",
    "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "\n",
    "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
    "\n",
    "        p3_in_origin = origin_1 - p_rotate[:, 4, :]\n",
    "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
    "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
    "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
    "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
    "\n",
    "        new_p_1 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
    "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
    "    else:\n",
    "        new_p_1 = np.zeros((0, 4, 2))\n",
    "    return np.concatenate([new_p_0, new_p_1])\n",
    "\n",
    "\n",
    "\n",
    "#Some geometrical functions used in codes\n",
    "def restore_rectangle(origin, geometry):\n",
    "    return restore_rectangle_rbox(origin, geometry)\n",
    "\n",
    "def getRotateRect(box):\n",
    "    rect = cv2.minAreaRect(box)\n",
    "\n",
    "    angle=rect[2]  # angle = [-90, 0)\n",
    "    if angle < -45:\n",
    "        rect = (rect[0], (rect[1][0], rect[1][1]), rect[2])\n",
    "        angle += 90\n",
    "        size = (rect[1][1],rect[1][0])\n",
    "    else:\n",
    "        rect = (rect[0], (rect[1][0], rect[1][1]), rect[2])\n",
    "        size=rect[1]\n",
    "\n",
    "    box_ = cv2.boxPoints(rect)\n",
    "    return np.concatenate([rect[0], size]), angle, box_\n",
    "\n",
    "\n",
    "#These Functions are used to Generate ROI params like out box,crop box & angles that we use to crop text from image\n",
    "def generate_roiRotatePara(box, angle, expand_w = 60):\n",
    "    '''Generate all ROI Parameterts'''\n",
    "    p0_rect, p1_rect, p2_rect, p3_rect = box\n",
    "    cxy = (p0_rect + p2_rect) / 2.\n",
    "    size = np.array([np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p0_rect - p3_rect)])\n",
    "    rrect = np.concatenate([cxy, size])\n",
    "\n",
    "    box=np.array(box)\n",
    "\n",
    "    points=np.array(box, dtype=np.int32)\n",
    "    xmin=np.min(points[:,0])\n",
    "    xmax=np.max(points[:,0])\n",
    "    ymin=np.min(points[:,1])\n",
    "    ymax=np.max(points[:,1])\n",
    "    bbox = np.array([xmin, ymin, xmax, ymax])\n",
    "    if np.any(bbox < -expand_w):\n",
    "        return None\n",
    "    \n",
    "    rrect[:2] -= bbox[:2]\n",
    "    rrect[:2] -= rrect[2:] / 2\n",
    "    rrect[2:] += rrect[:2]\n",
    "\n",
    "    bbox[2:] -= bbox[:2]\n",
    "\n",
    "    rrect[::2] = np.clip(rrect[::2], 0, bbox[2])\n",
    "    rrect[1::2] = np.clip(rrect[1::2], 0, bbox[3])\n",
    "    rrect[2:] -= rrect[:2]\n",
    "    \n",
    "    return bbox.astype(np.int32), rrect.astype(np.int32), - angle\n",
    "\n",
    "\n",
    "\n",
    "def restore_roiRotatePara(box):\n",
    "    rectange, rotate_angle = sort_rectangle(box)\n",
    "    return generate_roiRotatePara(rectange, rotate_angle)\n",
    "\n",
    "\n",
    "#This function is used to generate geo_map,score_map, training_mask,corp_box,out_box,angle that we use while training model\n",
    "def generate_rbox(im_size, polys, tags,num_classes):\n",
    "    '''Genrate score_map and geo_map for image'''\n",
    "    h, w = im_size\n",
    "    poly_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    score_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    geo_map = np.zeros((h, w, 5), dtype=np.float32)\n",
    "\n",
    "    outBoxs = []\n",
    "    cropBoxs = []\n",
    "    angles = []\n",
    "    text_tags = []\n",
    "    recg_masks = []\n",
    "    # mask used during traning, to ignore some hard areas\n",
    "    training_mask = np.ones((h, w), dtype=np.uint8)\n",
    "    for poly_idx, poly_tag in enumerate(zip(polys, tags)):\n",
    "        poly = poly_tag[0]\n",
    "        #print(poly)\n",
    "        tag = poly_tag[1]\n",
    "        #print(tag)\n",
    "        r = [None, None, None, None]\n",
    "        for i in range(4):\n",
    "            r[i] = min(np.linalg.norm(poly[i] - poly[(i + 1) % 4]),\n",
    "                       np.linalg.norm(poly[i] - poly[(i - 1) % 4]))\n",
    "        # score map\n",
    "        shrinked_poly = shrink_poly(poly.copy(), r).astype(np.int32)[np.newaxis, :, :]\n",
    "        cv2.fillPoly(score_map, shrinked_poly, 1)\n",
    "        cv2.fillPoly(poly_mask, shrinked_poly, poly_idx + 1)\n",
    "\n",
    "        # if geometry == 'RBOX':\n",
    "        # generate a parallelogram for any combination of two vertices\n",
    "        fitted_parallelograms = []\n",
    "        for i in range(4):\n",
    "            p0 = poly[i]\n",
    "            p1 = poly[(i + 1) % 4]\n",
    "            p2 = poly[(i + 2) % 4]\n",
    "            p3 = poly[(i + 3) % 4]\n",
    "            edge = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
    "            backward_edge = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
    "            forward_edge = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
    "            if point_dist_to_line(p0, p1, p2) > point_dist_to_line(p0, p1, p3):\n",
    "                #  parallel lines through p2\n",
    "                if edge[1] == 0:\n",
    "                    edge_opposite = [1, 0, -p2[0]]\n",
    "                else:\n",
    "                    edge_opposite = [edge[0], -1, p2[1] - edge[0] * p2[0]]\n",
    "            else:\n",
    "                # after p3\n",
    "                if edge[1] == 0:\n",
    "                    edge_opposite = [1, 0, -p3[0]]\n",
    "                else:\n",
    "                    edge_opposite = [edge[0], -1, p3[1] - edge[0] * p3[0]]\n",
    "            # move forward edge\n",
    "            new_p0 = p0\n",
    "            new_p1 = p1\n",
    "            new_p2 = p2\n",
    "            new_p3 = p3\n",
    "            new_p2 = line_cross_point(forward_edge, edge_opposite)\n",
    "            if point_dist_to_line(p1, new_p2, p0) > point_dist_to_line(p1, new_p2, p3):\n",
    "                # across p0\n",
    "                if forward_edge[1] == 0:\n",
    "                    forward_opposite = [1, 0, -p0[0]]\n",
    "                else:\n",
    "                    forward_opposite = [forward_edge[0], -1, p0[1] - forward_edge[0] * p0[0]]\n",
    "            else:\n",
    "                # across p3\n",
    "                if forward_edge[1] == 0:\n",
    "                    forward_opposite = [1, 0, -p3[0]]\n",
    "                else:\n",
    "                    forward_opposite = [forward_edge[0], -1, p3[1] - forward_edge[0] * p3[0]]\n",
    "            new_p0 = line_cross_point(forward_opposite, edge)\n",
    "            new_p3 = line_cross_point(forward_opposite, edge_opposite)\n",
    "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
    "            # or move backward edge\n",
    "            new_p0 = p0\n",
    "            new_p1 = p1\n",
    "            new_p2 = p2\n",
    "            new_p3 = p3\n",
    "            new_p3 = line_cross_point(backward_edge, edge_opposite)\n",
    "            if point_dist_to_line(p0, p3, p1) > point_dist_to_line(p0, p3, p2):\n",
    "                # across p1\n",
    "                if backward_edge[1] == 0:\n",
    "                    backward_opposite = [1, 0, -p1[0]]\n",
    "                else:\n",
    "                    backward_opposite = [backward_edge[0], -1, p1[1] - backward_edge[0] * p1[0]]\n",
    "            else:\n",
    "                # across p2\n",
    "                if backward_edge[1] == 0:\n",
    "                    backward_opposite = [1, 0, -p2[0]]\n",
    "                else:\n",
    "                    backward_opposite = [backward_edge[0], -1, p2[1] - backward_edge[0] * p2[0]]\n",
    "            new_p1 = line_cross_point(backward_opposite, edge)\n",
    "            new_p2 = line_cross_point(backward_opposite, edge_opposite)\n",
    "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
    "        areas = [Polygon(t).area for t in fitted_parallelograms]\n",
    "        parallelogram = np.array(fitted_parallelograms[np.argmin(areas)][:-1], dtype=np.float32)\n",
    "        # sort thie polygon\n",
    "        parallelogram_coord_sum = np.sum(parallelogram, axis=1)\n",
    "        min_coord_idx = np.argmin(parallelogram_coord_sum)\n",
    "        parallelogram = parallelogram[\n",
    "            [min_coord_idx, (min_coord_idx + 1) % 4, (min_coord_idx + 2) % 4, (min_coord_idx + 3) % 4]]\n",
    "\n",
    "        rectange = rectangle_from_parallelogram(parallelogram)\n",
    "        rectange, rotate_angle = sort_rectangle(rectange)\n",
    "\n",
    "        p0_rect, p1_rect, p2_rect, p3_rect = rectange\n",
    "\n",
    "        # if the poly is too small, then ignore it during training\n",
    "        poly_h = min(np.linalg.norm(p0_rect - p3_rect), np.linalg.norm(p1_rect - p2_rect))\n",
    "        poly_w = min(np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p2_rect - p3_rect))\n",
    "\n",
    "        invaild = (min(poly_h, poly_w) < 6) or tag is None or (True and poly_h > poly_w * 2)\n",
    "\n",
    "        if invaild:\n",
    "            cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
    "        xy_in_poly = np.argwhere(poly_mask == (poly_idx + 1))\n",
    "        \n",
    "        if not invaild:\n",
    "            roiRotatePara = generate_roiRotatePara(rectange, rotate_angle)\n",
    "            if roiRotatePara:\n",
    "                outBox, cropBox, angle = roiRotatePara\n",
    "                if min(cropBox[2:]) > 6:\n",
    "                    w , h = cropBox[2:]\n",
    "                    textImgW = np.ceil(min(w / float(h) * 32, 256) / 4 /1)\n",
    "                    #print(tag)\n",
    "                    if textImgW >= 2 * min(len(tag), 16):  # avoid CTC error\n",
    "                        outBoxs.append(outBox)\n",
    "                        cropBoxs.append(cropBox)\n",
    "                        angles.append(angle)\n",
    "                        text_tags.append(tag[:16])\n",
    "                        recg_masks.append(1.)\n",
    "\n",
    "        for y, x in xy_in_poly:\n",
    "            point = np.array([x, y], dtype=np.float32)\n",
    "            # top\n",
    "            geo_map[y, x, 0] = point_dist_to_line(p0_rect, p1_rect, point) + 3\n",
    "            # right\n",
    "            geo_map[y, x, 1] = point_dist_to_line(p1_rect, p2_rect, point) + 3\n",
    "            # down\n",
    "            geo_map[y, x, 2] = point_dist_to_line(p2_rect, p3_rect, point) + 3\n",
    "            # left\n",
    "            geo_map[y, x, 3] = point_dist_to_line(p3_rect, p0_rect, point) + 3\n",
    "            # angle\n",
    "            geo_map[y, x, 4] = rotate_angle\n",
    "    if len(outBoxs) == 0:\n",
    "        outBoxs.append([0, 0, 2 * 4, 2 * 4]) # keep extract From sharedConv feature map not zero\n",
    "        cropBoxs.append([0, 0, 2 * 4, 2 * 4])\n",
    "        angles.append(0.)\n",
    "        text_tags.append([num_classes - 2])\n",
    "        recg_masks.append(0.)\n",
    "\n",
    "    outBoxs = np.array(outBoxs, np.int32)\n",
    "    cropBoxs = np.array(cropBoxs, np.int32)\n",
    "    angles = np.array(angles, np.float32)\n",
    "\n",
    "    return score_map, geo_map, training_mask, (outBoxs, cropBoxs, angles), text_tags, recg_masks\n",
    "\n",
    "\n",
    "def sort_poly(p):\n",
    "    \"\"\"function to sort polygons\"\"\"\n",
    "    min_axis = np.argmin(np.sum(p, axis=1))\n",
    "    p = p[[min_axis, (min_axis+1)%4, (min_axis+2)%4, (min_axis+3)%4]]\n",
    "    if abs(p[0, 0] - p[1, 0]) > abs(p[0, 1] - p[1, 1]):\n",
    "        return p\n",
    "    else:\n",
    "        return p[[0, 3, 2, 1]]\n",
    "\n",
    "\n",
    "def intersection(g, p):\n",
    "    \"\"\"calculation intersection and union for IOU\"\"\"\n",
    "    g = Polygon(g[:8].reshape((4, 2)))\n",
    "    p = Polygon(p[:8].reshape((4, 2)))\n",
    "    if not g.is_valid or not p.is_valid:\n",
    "        return 0\n",
    "    inter = Polygon(g).intersection(Polygon(p)).area\n",
    "    union = g.area + p.area - inter\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return inter/union\n",
    "\n",
    "\n",
    "def weighted_merge(g, p):\n",
    "    g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8])\n",
    "    g[8] = (g[8] + p[8])\n",
    "    return g\n",
    "\n",
    "def standard_nms(S, thres):\n",
    "    order = np.argsort(S[:, 8])[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n",
    "\n",
    "        inds = np.where(ovr <= thres)[0]\n",
    "        order = order[inds+1]\n",
    "\n",
    "    return S[keep]\n",
    "\n",
    "\n",
    "def nms_locality(polys, thres=0.3):\n",
    "    '''\n",
    "    :param polys: a N*9 numpy array. first 8 coordinates, then prob\n",
    "    :return: boxes after nms\n",
    "    '''\n",
    "    S = []\n",
    "    p = None\n",
    "  \n",
    "    for g in polys:\n",
    "        if p is not None and intersection(g, p) > thres:\n",
    "        \n",
    "            p = weighted_merge(g, p)\n",
    "        else:\n",
    "            if p is not None:\n",
    "                S.append(p)\n",
    "            p = g\n",
    "  \n",
    "    if p is not None:\n",
    "        S.append(p)\n",
    "\n",
    "    if len(S) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    return standard_nms(np.array(S), thres)\n",
    "\n",
    "\n",
    "\n",
    "def total_inference(img):\n",
    "  '''This function is main complete pipeline of our Model'''\n",
    "  \n",
    "  #1.Text Detection\n",
    "  img=cv2.resize(img,(512,512))\n",
    "  ii=detector.predict(np.expand_dims(img,axis=0))\n",
    "  score_map=ii[0][:,:,0]\n",
    "  geo_map=ii[0][:,:,1:]\n",
    "  for ind in [0,1,2,3,4]:\n",
    "    geo_map[:,:,ind]*=score_map\n",
    "\n",
    "  #2.ROI Rotate  \n",
    "  score_map_thresh=0.5\n",
    "  box_thresh=0.1 \n",
    "  nms_thres=0.2\n",
    "  if len(score_map.shape) == 4:\n",
    "    score_map = score_map[0, :, :, 0]\n",
    "    geo_map = geo_map[0, :, :, :]\n",
    "\n",
    "  # filter the score map\n",
    "  xy_text = np.argwhere(score_map > score_map_thresh)\n",
    "\n",
    "  # sort the text boxes via the y axis\n",
    "  xy_text = xy_text[np.argsort(xy_text[:, 0])]\n",
    "\n",
    "  # restore\n",
    "  text_box_restored = restore_rectangle(xy_text[:, ::-1], geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2\n",
    "  boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32)\n",
    "  boxes[:, :8] = text_box_restored.reshape((-1, 8))\n",
    "  boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]]\n",
    "  boxes = nms_locality(boxes.astype(np.float64), nms_thres)\n",
    "  \n",
    "\n",
    "  # here we filter some low score boxes by the average score map, this is different from the orginal paper\n",
    "  for i, box in enumerate(boxes):\n",
    "    mask = np.zeros_like(score_map, dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32), 1)\n",
    "    boxes[i, 8] = cv2.mean(score_map, mask)[0]\n",
    "    if i==4:\n",
    "      break\n",
    "  if len(boxes)>0:\n",
    "    boxes = boxes[boxes[:, 8] > box_thresh]\n",
    "  boxes[:,:8:2] = np.clip(boxes[:,:8:2], 0, 512 - 1)\n",
    "  boxes[:,1:8:2] = np.clip(boxes[:,1:8:2], 0, 512 - 1)  \n",
    "  res = []\n",
    "  result = []\n",
    "  if len(boxes)>0:\n",
    "    for box in boxes:\n",
    "      box_ =  box[:8].reshape((4, 2))\n",
    "      if np.linalg.norm(box_[0] - box_[1]) < 8 or np.linalg.norm(box_[3]-box_[0]) < 8:\n",
    "        continue\n",
    "      result.append(box_)\n",
    "  res.append(np.array(result, np.float32))   \n",
    "\n",
    "  box_index = []\n",
    "  brotateParas = []\n",
    "  filter_bsharedFeatures = []\n",
    "  for i in range(len(res)):\n",
    "    rotateParas = []\n",
    "    rboxes=res[i]\n",
    "    txt=[]\n",
    "    for j, rbox in enumerate(rboxes):\n",
    "      para = restore_roiRotatePara(rbox)\n",
    "      if para and min(para[1][2:]) > 8:\n",
    "        rotateParas.append(para)\n",
    "        box_index.append((i, j))\n",
    "    pts=[]   \n",
    "    \n",
    "    \n",
    "    #3. Text Recognition (From boxes given by Text Detection+ROI Rotate) \n",
    "    if len(rotateParas) > 0:\n",
    "      for num in range(len(rotateParas)):\n",
    "        text=\"\"\n",
    "        out=rotateParas[num][0]\n",
    "        crop=rotateParas[num][1]\n",
    "        points=np.array([[out[0],out[1]],[out[0]+out[2],out[1]],[out[0]+out[2],out[1]+out[3]],[out[0],out[1]+out[3]]])\n",
    "        angle=rotateParas[num][2] \n",
    "        img1=tf.image.crop_to_bounding_box(img,out[1],out[0],out[3],out[2])\n",
    "        img2=tf.keras.preprocessing.image.random_rotation(img1,angle)\n",
    "        img2=tf.image.crop_to_bounding_box(img2,crop[1],crop[0],crop[3],crop[2]).numpy()\n",
    "        img2=cv2.resize(img2,(128,64))\n",
    "        img2=cv2.detailEnhance(img2)\n",
    "        ii=recognizer.predict(np.expand_dims(img2,axis=0))\n",
    "        arr=tf.keras.backend.ctc_decode(ii,np.ones((1),'int8')*64,)\n",
    "        for val in arr[0][0].numpy()[0]:\n",
    "          if val==-1:\n",
    "            break\n",
    "          else:\n",
    "            text+=index_to_char[val]\n",
    "        txt.append(text)\n",
    "        pts.append(points)\n",
    "    \n",
    "    # 4. Labeling detected and Recognized Text in Image  \n",
    "    for i in range(len(txt)):\n",
    "      cv2.polylines(img,[pts[i]],isClosed=True,color=(0,255,0),thickness=1)\n",
    "      cv2.putText(img,txt[i],(pts[i][0][0],pts[i][0][1]),cv2.FONT_HERSHEY_SIMPLEX,1, (0,250,0),2)  \n",
    "    return img,txt\n",
    "\n",
    "\n",
    "\n",
    "@st.cache(show_spinner=False)\n",
    "def detection():\n",
    "    class Deconv(tf.keras.layers.Layer):\n",
    "        def __init__(self,name=\"Deconv\"):\n",
    "            super().__init__(name)\n",
    "            self.inp_shape = 0\n",
    "            self.upsample = None\n",
    "            self.conv = None\n",
    "            self.bn = None\n",
    "        \n",
    "        def build(self,imshape):\n",
    "            self.inp_shape=imshape\n",
    "            self.upsample=UpSampling2D(size=(2,2),interpolation='bilinear',\n",
    "                                    data_format='channels_last',)\n",
    "            self.conv=Conv2D(filters=self.inp_shape[-1]//2,kernel_size=3,\n",
    "                            padding='same',activation='relu',\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal(seed=32),\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(1e-5), \n",
    "                            use_bias=False)\n",
    "            self.bn=BatchNormalization()\n",
    "   \n",
    "        def call(self,X):\n",
    "        \n",
    "            x=self.upsample(X)\n",
    "            x=self.conv(x)\n",
    "            x=self.bn(x)\n",
    "            return x\n",
    "    # Detection Branch - Modeling with pretrained ResNet50 and Deconv layer\n",
    "\n",
    "    # Shared network \n",
    "\n",
    "    resnet=tf.keras.applications.ResNet50(input_shape=(512,512,3),\n",
    "                                        include_top=False,\n",
    "                                        weights='imagenet')\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    for layers in resnet.layers:\n",
    "        layers.trainable=False\n",
    "\n",
    "    x = resnet.get_layer('conv5_block3_out').output\n",
    "\n",
    "    x = Deconv('Deconv1')(x)\n",
    "    x = tf.keras.layers.add([x,resnet.get_layer('conv4_block6_out').output])\n",
    "\n",
    "    x = Deconv('Deconv2')(x)\n",
    "    x = tf.keras.layers.add([x,resnet.get_layer('conv3_block4_out').output])\n",
    "\n",
    "    x = Deconv('Deconv3')(x)\n",
    "    x = tf.keras.layers.add([x,resnet.get_layer('conv2_block3_out').output])\n",
    "\n",
    "    # --- end ---\n",
    "\n",
    "    # text detector specific\n",
    "    x=BatchNormalization()(x)\n",
    "    x = Deconv('Deconv4')(x)\n",
    "    x = Deconv('Deconv5')(x)\n",
    "\n",
    "    score=Conv2D(1,kernel_size=3,padding='same',activation='sigmoid')(x)\n",
    "\n",
    "    # Used this beacause sigmoid gives values in range of 0-1(as mentioned in git repository)\n",
    "    geo_map=Conv2D(4,kernel_size=3,padding='same',activation='sigmoid')(x)*512\n",
    "\n",
    "    # Angles are assumed to be between [-45 to 45]\n",
    "    angle_map=(Conv2D(1,kernel_size=3,padding='same',activation='sigmoid')(x)-0.5)*np.pi/2\n",
    "\n",
    "    out=tf.concat([score,geo_map,angle_map],axis=3)\n",
    "\n",
    "    detector=tf.keras.Model(resnet.input,out,name='detector')\n",
    "\n",
    "    #Loading the saved weights of detection model\n",
    "    detector.load_weights('/content/gdrive/MyDrive/Colab Notebooks/Case_Study_2/detector_10k_ICDAR.h5')\n",
    "    return detector\n",
    "\n",
    "\n",
    "@st.cache(show_spinner=False)\n",
    "def recognition():\n",
    "    def conv_bn_relu(input,n_filters,k_size=(3,3),strd=1,pad='same'):\n",
    "        \"\"\"function to do conv--> batch-norm-->Relu activation\"\"\"\n",
    "        temp=input\n",
    "        conv = Conv2D(filters=n_filters, kernel_size=k_size,\n",
    "                    strides=strd, padding =pad,\n",
    "                    activation = 'relu',\n",
    "                    kernel_initializer = tf.keras.initializers.he_normal(seed=0))(temp)\n",
    "        bn=BatchNormalization()(conv)\n",
    "        relu=Activation('relu')(bn)\n",
    "\n",
    "        return relu\n",
    "\n",
    "    chars = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZÉ´-~`<>'.:;^/|!?$%#@&*()[]{}_+=,\\\\\\\"\"\n",
    "    n_classes = len(chars) \n",
    "    char_to_index={}\n",
    "    index_to_char={}\n",
    "    for i,char in enumerate(chars):\n",
    "        index_to_char[i+1]=char\n",
    "        char_to_index[char]=i+1\n",
    "\n",
    "    # Recognition Model\n",
    "\n",
    "    input = Input(shape = (64,128,3), dtype='float32',name ='input')\n",
    "\n",
    "    x = conv_bn_relu(input,64,(3,3),1,'same')\n",
    "    x = MaxPool2D(pool_size = (2, 1),strides=(2,1),padding='same')(x)\n",
    "\n",
    "    x = conv_bn_relu(x,64,(3,3),1,'same')\n",
    "    x = MaxPool2D(pool_size = (2, 1),strides=(2,1),padding='same')(x)\n",
    "\n",
    "\n",
    "    x = conv_bn_relu(x,32,(3,3),1,'same')\n",
    "    x = conv_bn_relu(x,32,(3,3),1,'same')\n",
    "    x = MaxPool2D(pool_size = (2, 1),strides=(2,1),padding='same')(x)\n",
    "\n",
    "    x = conv_bn_relu(x,32,(3,3),1,'same')\n",
    "    x = conv_bn_relu(x,32,(3,3),1,'same')\n",
    "    x = MaxPool2D(pool_size = (2, 1),strides=(2,1),padding='same')(x)\n",
    "\n",
    "\n",
    "    x = conv_bn_relu(x,64,(3,3),1,'same')\n",
    "    x = Reshape(target_shape= ((64,-1)))(x)\n",
    "    x= Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "\n",
    "\n",
    "    x= Dropout(rate=0.1)(x)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True,go_backwards=True))(x)\n",
    "\n",
    "\n",
    "    x= Dropout(rate=0.1)(x)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True,go_backwards=True))(x)\n",
    "\n",
    "\n",
    "    x= Dropout(rate=0.1)(x)\n",
    "    out = Dense(units=n_classes+2,activation='softmax',\n",
    "                kernel_initializer=tf.keras.initializers.glorot_normal(seed=0))(x)\n",
    "\n",
    "    recognizer=Model(input, out)\n",
    "    recognizer.load_weights('/content/gdrive/MyDrive/Colab Notebooks/Case_Study_2/recognizer_1.h5')\n",
    "    return recognizer\n",
    "\n",
    "\n",
    "\n",
    "detector = detection()\n",
    "recognizer = recognition()\n",
    "\n",
    "chars = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZÉ´-~`<>'.:;^/|!?$%#@&*()[]{}_+=,\\\\\\\"\"\n",
    "n_classes = len(chars) \n",
    "char_to_index={}\n",
    "index_to_char={}\n",
    "for i,char in enumerate(chars):\n",
    "  index_to_char[i+1]=char\n",
    "  char_to_index[char]=i+1\n",
    "\n",
    "\n",
    "st.title('FOTS - Fast Oriented Text Spotting with an Unified Network')\n",
    "st.header('Text Detection & Recognition')\n",
    "st.subheader('Upload an image to process')\n",
    "img=st.file_uploader('Upload an Image',type=['jpg', 'png'])\n",
    "if img:\n",
    "  st.info('Image Uploaded...')    \n",
    "  st.image(img,width=512)\n",
    "  image = Image.open(img)\n",
    "  img = np.array(image)\n",
    "  st.info('Processing...')\n",
    "  try:\n",
    "      im,txt=total_inference(img)\n",
    "      txt=','.join(txt)\n",
    "      im=cv2.resize(im,(512,400))\n",
    "      st.header('Result')\n",
    "      st.image(im,caption=txt)\n",
    "  except Exception as e:\n",
    "      st.error('Some error occured. Try with another image.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "Z8VLLSka6ke0",
    "outputId": "011af1e9-64ef-4188-9de7-6af13f336ebf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-29 21:39:41.696 ngrok process starting: 1604\n",
      "2021-07-29 21:39:41.723 t=2021-07-29T21:39:41+0000 lvl=info msg=\"no configuration paths supplied\"\n",
      "\n",
      "2021-07-29 21:39:41.728 t=2021-07-29T21:39:41+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
      "\n",
      "2021-07-29 21:39:41.735 t=2021-07-29T21:39:41+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
      "\n",
      "2021-07-29 21:39:41.748 t=2021-07-29T21:39:41+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
      "\n",
      "2021-07-29 21:39:41.817 t=2021-07-29T21:39:41+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
      "\n",
      "2021-07-29 21:39:41.822 t=2021-07-29T21:39:41+0000 lvl=info msg=\"client session established\" obj=csess id=192dcdae0647\n",
      "\n",
      "2021-07-29 21:39:41.829 ngrok process has started: http://127.0.0.1:4040\n",
      "2021-07-29 21:39:41.841 t=2021-07-29T21:39:41+0000 lvl=info msg=start pg=/api/tunnels id=8be17b4417689ac5\n",
      "\n",
      "2021-07-29 21:39:41.847 t=2021-07-29T21:39:41+0000 lvl=info msg=end pg=/api/tunnels id=8be17b4417689ac5 status=200 dur=388.927µs\n",
      "\n",
      "2021-07-29 21:39:41.850 t=2021-07-29T21:39:41+0000 lvl=info msg=start pg=/api/tunnels id=1f6ec8c315ea5a75\n",
      "\n",
      "2021-07-29 21:39:41.852 t=2021-07-29T21:39:41+0000 lvl=info msg=end pg=/api/tunnels id=1f6ec8c315ea5a75 status=200 dur=127.912µs\n",
      "\n",
      "2021-07-29 21:39:41.857 t=2021-07-29T21:39:41+0000 lvl=info msg=start pg=/api/tunnels id=40566030d0d1aff4\n",
      "\n",
      "2021-07-29 21:39:41.883 t=2021-07-29T21:39:41+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8501-195e7d39-4aa8-4a36-b5ac-551a0f37b1a4 (http)\" addr=http://localhost:8501 url=http://3b1ec9197b77.ngrok.io\n",
      "\n",
      "2021-07-29 21:39:41.888 t=2021-07-29T21:39:41+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-195e7d39-4aa8-4a36-b5ac-551a0f37b1a4 addr=http://localhost:8501 url=https://3b1ec9197b77.ngrok.io\n",
      "\n",
      "2021-07-29 21:39:41.892 t=2021-07-29T21:39:41+0000 lvl=info msg=end pg=/api/tunnels id=40566030d0d1aff4 status=201 dur=40.027292ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'http://3b1ec9197b77.ngrok.io'"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!streamlit run app.py &>/dev/null&\n",
    "public_url = ngrok.connect(port='8501')\n",
    "public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6g6U3l06ny8",
    "outputId": "accfc7e3-5de0-4fb1-91e6-2cd23084ed77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-29 21:44:24.801 Killing ngrok process: 1604\n"
     ]
    }
   ],
   "source": [
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJpm1p5DChJZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Case_Study_2_Deployment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
